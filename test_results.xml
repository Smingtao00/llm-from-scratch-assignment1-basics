<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="10" skipped="1" tests="48" time="40.068" timestamp="2026-01-31T22:32:08.008395+08:00" hostname="smt00"><testcase classname="tests.test_data" name="test_get_batch" time="0.000"><failure message="NotImplementedError">def test_get_batch():
        dataset = np.arange(0, 100)
        context_length = 7
        batch_size = 32
        device = "cpu"
    
        # Sanity check to make sure that the random samples are indeed somewhat random.
        starting_indices = Counter()
        num_iters = 1000
        for _ in range(num_iters):
&gt;           x, y = run_get_batch(
                dataset=dataset,
                batch_size=batch_size,
                context_length=context_length,
                device=device,
            )

tests/test_data.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

dataset = array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
       17, 18, 19, 20, 21, 22, 23, 24, 25, ...72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,
       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])
batch_size = 32, context_length = 7, device = 'cpu'

    def run_get_batch(
        dataset: npt.NDArray, batch_size: int, context_length: int, device: str
    ) -&gt; tuple[torch.Tensor, torch.Tensor]:
        """
        Given a dataset (a 1D numpy array of integers) and a desired batch size and
        context length, sample language modeling input sequences and their corresponding
        labels from the dataset.
    
        Args:
            dataset (np.array): 1D numpy array of integer token IDs in the dataset.
            batch_size (int): Desired batch size to sample.
            context_length (int): Desired context length of each sampled example.
            device (str): PyTorch device string (e.g., 'cpu' or 'cuda:0') indicating the device
                to place the sampled input sequences and labels on.
    
        Returns:
            Tuple of torch.LongTensors of shape (batch_size, context_length). The first tuple item
            is the sampled input sequences, and the second tuple item is the corresponding
            language modeling labels.
        """
&gt;       raise NotImplementedError
E       NotImplementedError

tests/adapters.py:279: NotImplementedError</failure></testcase><testcase classname="tests.test_model" name="test_linear" time="0.075"><failure message="RuntimeError: Error(s) in loading state_dict for Linear:&#10;&#09;Missing key(s) in state_dict: &quot;weight&quot;. &#10;&#09;Unexpected key(s) in state_dict: &quot;weights&quot;.">numpy_snapshot = &lt;tests.conftest.NumpySnapshot object at 0x7ebdb93523c0&gt;
ts_state_dict = ({'layers.0.attn.k_proj.weight': tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],
        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {'context_length': 16, 'd_ff': 128, 'd_model': 64, 'ffn_type': None, ...})
in_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],
         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],
         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])
d_model = 64, d_ff = 128

    def test_linear(numpy_snapshot, ts_state_dict, in_embeddings, d_model, d_ff):
        w1_weight = ts_state_dict[0]["layers.0.ffn.w1.weight"]
&gt;       output = run_linear(
            d_in=d_model,
            d_out=d_ff,
            weights=w1_weight,
            in_features=in_embeddings,
        )

tests/test_model.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/adapters.py:22: in run_linear
    linear_module.load_state_dict(state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Linear()
state_dict = OrderedDict({'weights': tensor([[ 0.0547,  0.0756,  0.0423,  ...,  0.1840,  0.0190, -0.0732],
        [-0.0683,  0.048...42, -0.0789,  ...,  0.0045, -0.1143,  0.0156],
        [-0.1401, -0.1757, -0.0704,  ..., -0.0199, -0.0032, -0.0454]])})
strict = True, assign = False

    def load_state_dict(
        self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False
    ):
        r"""Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.
    
        If :attr:`strict` is ``True``, then
        the keys of :attr:`state_dict` must exactly match the keys returned
        by this module's :meth:`~torch.nn.Module.state_dict` function.
    
        .. warning::
            If :attr:`assign` is ``True`` the optimizer must be created after
            the call to :attr:`load_state_dict` unless
            :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.
    
        Args:
            state_dict (dict): a dict containing parameters and
                persistent buffers.
            strict (bool, optional): whether to strictly enforce that the keys
                in :attr:`state_dict` match the keys returned by this module's
                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``
            assign (bool, optional): When set to ``False``, the properties of the tensors
                in the current module are preserved whereas setting it to ``True`` preserves
                properties of the Tensors in the state dict. The only
                exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`
                for which the value from the module is preserved. Default: ``False``
    
        Returns:
            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
                * ``missing_keys`` is a list of str containing any keys that are expected
                    by this module but missing from the provided ``state_dict``.
                * ``unexpected_keys`` is a list of str containing the keys that are not
                    expected by this module but present in the provided ``state_dict``.
    
        Note:
            If a parameter or buffer is registered as ``None`` and its corresponding key
            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a
            ``RuntimeError``.
        """
        if not isinstance(state_dict, Mapping):
            raise TypeError(
                f"Expected state_dict to be dict-like, got {type(state_dict)}."
            )
    
        missing_keys: list[str] = []
        unexpected_keys: list[str] = []
        error_msgs: list[str] = []
    
        # copy state_dict so _load_from_state_dict can modify it
        metadata = getattr(state_dict, "_metadata", None)
        state_dict = OrderedDict(state_dict)
        if metadata is not None:
            # mypy isn't aware that "_metadata" exists in state_dict
            state_dict._metadata = metadata  # type: ignore[attr-defined]
    
        def load(module, local_state_dict, prefix="") -&gt; None:
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            if assign:
                local_metadata["assign_to_params_buffers"] = assign
            module._load_from_state_dict(
                local_state_dict,
                prefix,
                local_metadata,
                True,
                missing_keys,
                unexpected_keys,
                error_msgs,
            )
            for name, child in module._modules.items():
                if child is not None:
                    child_prefix = prefix + name + "."
                    child_state_dict = {
                        k: v
                        for k, v in local_state_dict.items()
                        if k.startswith(child_prefix)
                    }
                    load(child, child_state_dict, child_prefix)  # noqa: F821
    
            # Note that the hook can modify missing_keys and unexpected_keys.
            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)
            for hook in module._load_state_dict_post_hooks.values():
                out = hook(module, incompatible_keys)
                assert out is None, (
                    "Hooks registered with ``register_load_state_dict_post_hook`` are not"
                    "expected to return new values, if incompatible_keys need to be modified,"
                    "it should be done inplace."
                )
    
        load(self, state_dict)
        del load
    
        if strict:
            if len(unexpected_keys) &gt; 0:
                error_msgs.insert(
                    0,
                    "Unexpected key(s) in state_dict: {}. ".format(
                        ", ".join(f'"{k}"' for k in unexpected_keys)
                    ),
                )
            if len(missing_keys) &gt; 0:
                error_msgs.insert(
                    0,
                    "Missing key(s) in state_dict: {}. ".format(
                        ", ".join(f'"{k}"' for k in missing_keys)
                    ),
                )
    
        if len(error_msgs) &gt; 0:
&gt;           raise RuntimeError(
                "Error(s) in loading state_dict for {}:\n\t{}".format(
                    self.__class__.__name__, "\n\t".join(error_msgs)
                )
            )
E           RuntimeError: Error(s) in loading state_dict for Linear:
E           	Missing key(s) in state_dict: "weight". 
E           	Unexpected key(s) in state_dict: "weights".

.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629: RuntimeError</failure></testcase><testcase classname="tests.test_model" name="test_embedding" time="0.039" /><testcase classname="tests.test_model" name="test_swiglu" time="0.023" /><testcase classname="tests.test_model" name="test_scaled_dot_product_attention" time="0.011" /><testcase classname="tests.test_model" name="test_4d_scaled_dot_product_attention" time="1.563" /><testcase classname="tests.test_model" name="test_multihead_self_attention" time="0.025"><failure message="RuntimeError: Error(s) in loading state_dict for multihead_self_attention:&#10;&#09;Missing key(s) in state_dict: &quot;output_proj.weight&quot;. &#10;&#09;Unexpected key(s) in state_dict: &quot;o_proj.weight&quot;.">numpy_snapshot = &lt;tests.conftest.NumpySnapshot object at 0x7ebdb93e43b0&gt;
in_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],
         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],
         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])
d_model = 64, n_heads = 4
ts_state_dict = ({'layers.0.attn.k_proj.weight': tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],
        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {'context_length': 16, 'd_ff': 128, 'd_model': 64, 'ffn_type': None, ...})

    def test_multihead_self_attention(numpy_snapshot, in_embeddings, d_model, n_heads, ts_state_dict):
        d, _ = ts_state_dict
        q_proj_weight, k_proj_weight, v_proj_weight, o_proj_weight = [
            d[f"layers.0.attn.{k}_proj.weight"] for k in ["q", "k", "v", "output"]
        ]
&gt;       actual_output = run_multihead_self_attention(
            d_model=d_model,
            num_heads=n_heads,
            q_proj_weight=q_proj_weight,
            k_proj_weight=k_proj_weight,
            v_proj_weight=v_proj_weight,
            o_proj_weight=o_proj_weight,
            in_features=in_embeddings,
        )

tests/test_model.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/adapters.py:77: in run_multihead_self_attention
    msa.load_state_dict(state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = multihead_self_attention(
  (q_proj): Linear()
  (k_proj): Linear()
  (v_proj): Linear()
  (output_proj): Linear()
)
state_dict = OrderedDict({'q_proj.weight': tensor([[ 0.0372, -0.0888,  0.2558,  ...,  0.0049, -0.2693, -0.1108],
        [ 0.0520, ...95,  0.0148,  ...,  0.0888,  0.0211,  0.0004],
        [ 0.0432,  0.0543,  0.0269,  ..., -0.0546, -0.0417, -0.0533]])})
strict = True, assign = False

    def load_state_dict(
        self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False
    ):
        r"""Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.
    
        If :attr:`strict` is ``True``, then
        the keys of :attr:`state_dict` must exactly match the keys returned
        by this module's :meth:`~torch.nn.Module.state_dict` function.
    
        .. warning::
            If :attr:`assign` is ``True`` the optimizer must be created after
            the call to :attr:`load_state_dict` unless
            :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.
    
        Args:
            state_dict (dict): a dict containing parameters and
                persistent buffers.
            strict (bool, optional): whether to strictly enforce that the keys
                in :attr:`state_dict` match the keys returned by this module's
                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``
            assign (bool, optional): When set to ``False``, the properties of the tensors
                in the current module are preserved whereas setting it to ``True`` preserves
                properties of the Tensors in the state dict. The only
                exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`
                for which the value from the module is preserved. Default: ``False``
    
        Returns:
            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
                * ``missing_keys`` is a list of str containing any keys that are expected
                    by this module but missing from the provided ``state_dict``.
                * ``unexpected_keys`` is a list of str containing the keys that are not
                    expected by this module but present in the provided ``state_dict``.
    
        Note:
            If a parameter or buffer is registered as ``None`` and its corresponding key
            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a
            ``RuntimeError``.
        """
        if not isinstance(state_dict, Mapping):
            raise TypeError(
                f"Expected state_dict to be dict-like, got {type(state_dict)}."
            )
    
        missing_keys: list[str] = []
        unexpected_keys: list[str] = []
        error_msgs: list[str] = []
    
        # copy state_dict so _load_from_state_dict can modify it
        metadata = getattr(state_dict, "_metadata", None)
        state_dict = OrderedDict(state_dict)
        if metadata is not None:
            # mypy isn't aware that "_metadata" exists in state_dict
            state_dict._metadata = metadata  # type: ignore[attr-defined]
    
        def load(module, local_state_dict, prefix="") -&gt; None:
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            if assign:
                local_metadata["assign_to_params_buffers"] = assign
            module._load_from_state_dict(
                local_state_dict,
                prefix,
                local_metadata,
                True,
                missing_keys,
                unexpected_keys,
                error_msgs,
            )
            for name, child in module._modules.items():
                if child is not None:
                    child_prefix = prefix + name + "."
                    child_state_dict = {
                        k: v
                        for k, v in local_state_dict.items()
                        if k.startswith(child_prefix)
                    }
                    load(child, child_state_dict, child_prefix)  # noqa: F821
    
            # Note that the hook can modify missing_keys and unexpected_keys.
            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)
            for hook in module._load_state_dict_post_hooks.values():
                out = hook(module, incompatible_keys)
                assert out is None, (
                    "Hooks registered with ``register_load_state_dict_post_hook`` are not"
                    "expected to return new values, if incompatible_keys need to be modified,"
                    "it should be done inplace."
                )
    
        load(self, state_dict)
        del load
    
        if strict:
            if len(unexpected_keys) &gt; 0:
                error_msgs.insert(
                    0,
                    "Unexpected key(s) in state_dict: {}. ".format(
                        ", ".join(f'"{k}"' for k in unexpected_keys)
                    ),
                )
            if len(missing_keys) &gt; 0:
                error_msgs.insert(
                    0,
                    "Missing key(s) in state_dict: {}. ".format(
                        ", ".join(f'"{k}"' for k in missing_keys)
                    ),
                )
    
        if len(error_msgs) &gt; 0:
&gt;           raise RuntimeError(
                "Error(s) in loading state_dict for {}:\n\t{}".format(
                    self.__class__.__name__, "\n\t".join(error_msgs)
                )
            )
E           RuntimeError: Error(s) in loading state_dict for multihead_self_attention:
E           	Missing key(s) in state_dict: "output_proj.weight". 
E           	Unexpected key(s) in state_dict: "o_proj.weight".

.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629: RuntimeError</failure></testcase><testcase classname="tests.test_model" name="test_multihead_self_attention_with_rope" time="0.023"><failure message="RuntimeError: Error(s) in loading state_dict for multihead_self_attention:&#10;&#09;Missing key(s) in state_dict: &quot;output_proj.weight&quot;. &#10;&#09;Unexpected key(s) in state_dict: &quot;o_proj.weight&quot;.">numpy_snapshot = &lt;tests.conftest.NumpySnapshot object at 0x7ebdb951b570&gt;
in_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],
         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],
         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])
d_model = 64, n_heads = 4
ts_state_dict = ({'layers.0.attn.k_proj.weight': tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],
        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {'context_length': 16, 'd_ff': 128, 'd_model': 64, 'ffn_type': None, ...})
n_keys = 16, theta = 10000.0
pos_ids = tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])

    def test_multihead_self_attention_with_rope(
        numpy_snapshot, in_embeddings, d_model, n_heads, ts_state_dict, n_keys, theta, pos_ids
    ):
        d, _ = ts_state_dict
        q_proj_weight, k_proj_weight, v_proj_weight, o_proj_weight = [
            d[f"layers.0.attn.{k}_proj.weight"] for k in ["q", "k", "v", "output"]
        ]
        pos_ids = rearrange(pos_ids, "seq -&gt; 1 seq")
&gt;       actual_output = run_multihead_self_attention_with_rope(
            d_model=d_model,
            num_heads=n_heads,
            max_seq_len=n_keys,
            theta=theta,
            q_proj_weight=q_proj_weight,
            k_proj_weight=k_proj_weight,
            v_proj_weight=v_proj_weight,
            o_proj_weight=o_proj_weight,
            in_features=in_embeddings,
            token_positions=pos_ids,
        )

tests/test_model.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/adapters.py:106: in run_multihead_self_attention_with_rope
    msa.load_state_dict(state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = multihead_self_attention(
  (q_proj): Linear()
  (k_proj): Linear()
  (v_proj): Linear()
  (output_proj): Linear()
  (rope): RoPE()
)
state_dict = OrderedDict({'q_proj.weight': tensor([[ 0.0372, -0.0888,  0.2558,  ...,  0.0049, -0.2693, -0.1108],
        [ 0.0520, ...95,  0.0148,  ...,  0.0888,  0.0211,  0.0004],
        [ 0.0432,  0.0543,  0.0269,  ..., -0.0546, -0.0417, -0.0533]])})
strict = True, assign = False

    def load_state_dict(
        self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False
    ):
        r"""Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.
    
        If :attr:`strict` is ``True``, then
        the keys of :attr:`state_dict` must exactly match the keys returned
        by this module's :meth:`~torch.nn.Module.state_dict` function.
    
        .. warning::
            If :attr:`assign` is ``True`` the optimizer must be created after
            the call to :attr:`load_state_dict` unless
            :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.
    
        Args:
            state_dict (dict): a dict containing parameters and
                persistent buffers.
            strict (bool, optional): whether to strictly enforce that the keys
                in :attr:`state_dict` match the keys returned by this module's
                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``
            assign (bool, optional): When set to ``False``, the properties of the tensors
                in the current module are preserved whereas setting it to ``True`` preserves
                properties of the Tensors in the state dict. The only
                exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`
                for which the value from the module is preserved. Default: ``False``
    
        Returns:
            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
                * ``missing_keys`` is a list of str containing any keys that are expected
                    by this module but missing from the provided ``state_dict``.
                * ``unexpected_keys`` is a list of str containing the keys that are not
                    expected by this module but present in the provided ``state_dict``.
    
        Note:
            If a parameter or buffer is registered as ``None`` and its corresponding key
            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a
            ``RuntimeError``.
        """
        if not isinstance(state_dict, Mapping):
            raise TypeError(
                f"Expected state_dict to be dict-like, got {type(state_dict)}."
            )
    
        missing_keys: list[str] = []
        unexpected_keys: list[str] = []
        error_msgs: list[str] = []
    
        # copy state_dict so _load_from_state_dict can modify it
        metadata = getattr(state_dict, "_metadata", None)
        state_dict = OrderedDict(state_dict)
        if metadata is not None:
            # mypy isn't aware that "_metadata" exists in state_dict
            state_dict._metadata = metadata  # type: ignore[attr-defined]
    
        def load(module, local_state_dict, prefix="") -&gt; None:
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            if assign:
                local_metadata["assign_to_params_buffers"] = assign
            module._load_from_state_dict(
                local_state_dict,
                prefix,
                local_metadata,
                True,
                missing_keys,
                unexpected_keys,
                error_msgs,
            )
            for name, child in module._modules.items():
                if child is not None:
                    child_prefix = prefix + name + "."
                    child_state_dict = {
                        k: v
                        for k, v in local_state_dict.items()
                        if k.startswith(child_prefix)
                    }
                    load(child, child_state_dict, child_prefix)  # noqa: F821
    
            # Note that the hook can modify missing_keys and unexpected_keys.
            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)
            for hook in module._load_state_dict_post_hooks.values():
                out = hook(module, incompatible_keys)
                assert out is None, (
                    "Hooks registered with ``register_load_state_dict_post_hook`` are not"
                    "expected to return new values, if incompatible_keys need to be modified,"
                    "it should be done inplace."
                )
    
        load(self, state_dict)
        del load
    
        if strict:
            if len(unexpected_keys) &gt; 0:
                error_msgs.insert(
                    0,
                    "Unexpected key(s) in state_dict: {}. ".format(
                        ", ".join(f'"{k}"' for k in unexpected_keys)
                    ),
                )
            if len(missing_keys) &gt; 0:
                error_msgs.insert(
                    0,
                    "Missing key(s) in state_dict: {}. ".format(
                        ", ".join(f'"{k}"' for k in missing_keys)
                    ),
                )
    
        if len(error_msgs) &gt; 0:
&gt;           raise RuntimeError(
                "Error(s) in loading state_dict for {}:\n\t{}".format(
                    self.__class__.__name__, "\n\t".join(error_msgs)
                )
            )
E           RuntimeError: Error(s) in loading state_dict for multihead_self_attention:
E           	Missing key(s) in state_dict: "output_proj.weight". 
E           	Unexpected key(s) in state_dict: "o_proj.weight".

.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629: RuntimeError</failure></testcase><testcase classname="tests.test_model" name="test_transformer_lm" time="0.078" /><testcase classname="tests.test_model" name="test_transformer_lm_truncated_input" time="0.034" /><testcase classname="tests.test_model" name="test_transformer_block" time="0.011" /><testcase classname="tests.test_model" name="test_rmsnorm" time="0.008" /><testcase classname="tests.test_model" name="test_rope" time="0.003" /><testcase classname="tests.test_model" name="test_silu_matches_pytorch" time="0.002"><failure message="NotImplementedError">def test_silu_matches_pytorch():
        x = torch.tensor(
            [
                [0.2352, 0.9259, 0.5189, 0.4725, 0.9730],
                [0.7581, 0.9692, 0.2129, 0.9345, 0.0149],
            ]
        )
        expected_output = F.silu(x)
&gt;       actual_output = run_silu(x)
                        ^^^^^^^^^^^

tests/test_model.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

in_features = tensor([[0.2352, 0.9259, 0.5189, 0.4725, 0.9730],
        [0.7581, 0.9692, 0.2129, 0.9345, 0.0149]])

    def run_silu(in_features: Float[Tensor, " ..."]) -&gt; Float[Tensor, " ..."]:
        """Given a tensor of inputs, return the output of applying SiLU
        to each element.
    
        Args:
            in_features(Float[Tensor, "..."]): Input features to run SiLU on. Shape is arbitrary.
    
        Returns:
            Float[Tensor,"..."]: of with the same shape as `in_features` with the output of applying
            SiLU to each element.
        """
&gt;       raise NotImplementedError
E       NotImplementedError

tests/adapters.py:256: NotImplementedError</failure></testcase><testcase classname="tests.test_nn_utils" name="test_softmax_matches_pytorch" time="0.002" /><testcase classname="tests.test_nn_utils" name="test_cross_entropy" time="0.002"><failure message="NotImplementedError">def test_cross_entropy():
        inputs = torch.tensor(
            [
                [
                    [0.1088, 0.1060, 0.6683, 0.5131, 0.0645],
                    [0.4538, 0.6852, 0.2520, 0.3792, 0.2675],
                    [0.4578, 0.3357, 0.6384, 0.0481, 0.5612],
                    [0.9639, 0.8864, 0.1585, 0.3038, 0.0350],
                ],
                [
                    [0.3356, 0.9013, 0.7052, 0.8294, 0.8334],
                    [0.6333, 0.4434, 0.1428, 0.5739, 0.3810],
                    [0.9476, 0.5917, 0.7037, 0.2987, 0.6208],
                    [0.8541, 0.1803, 0.2054, 0.4775, 0.8199],
                ],
            ]
        )
        targets = torch.tensor([[1, 0, 2, 2], [4, 1, 4, 0]])
        expected = F.cross_entropy(inputs.view(-1, inputs.size(-1)), targets.view(-1))
        numpy.testing.assert_allclose(
&gt;           run_cross_entropy(inputs.view(-1, inputs.size(-1)), targets.view(-1)).detach().numpy(),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            expected.detach().numpy(),
            atol=1e-4,
        )

tests/test_nn_utils.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[0.1088, 0.1060, 0.6683, 0.5131, 0.0645],
        [0.4538, 0.6852, 0.2520, 0.3792, 0.2675],
        [0.4578, 0..., 0.5739, 0.3810],
        [0.9476, 0.5917, 0.7037, 0.2987, 0.6208],
        [0.8541, 0.1803, 0.2054, 0.4775, 0.8199]])
targets = tensor([1, 0, 2, 2, 4, 1, 4, 0])

    def run_cross_entropy(
        inputs: Float[Tensor, " batch_size vocab_size"], targets: Int[Tensor, " batch_size"]
    ) -&gt; Float[Tensor, ""]:
        """Given a tensor of inputs and targets, compute the average cross-entropy
        loss across examples.
    
        Args:
            inputs (Float[Tensor, "batch_size vocab_size"]): inputs[i][j] is the
                unnormalized logit of jth class for the ith example.
            targets (Int[Tensor, "batch_size"]): Tensor of shape (batch_size,) with the index of the correct class.
                Each value must be between 0 and `num_classes - 1`.
    
        Returns:
            Float[Tensor, ""]: The average cross-entropy loss across examples.
        """
&gt;       raise NotImplementedError
E       NotImplementedError

tests/adapters.py:300: NotImplementedError</failure></testcase><testcase classname="tests.test_nn_utils" name="test_gradient_clipping" time="0.210"><failure message="NotImplementedError">def test_gradient_clipping():
        tensors = [torch.randn((5, 5)) for _ in range(6)]
        max_norm = 1e-2
    
        t1 = tuple(torch.nn.Parameter(torch.clone(t)) for t in tensors)
        # Test freezing one parameter.
        t1[-1].requires_grad_(False)
    
        loss = torch.cat(t1).sum()
        loss.backward()
        clip_grad_norm_(t1, max_norm)
        t1_grads = [torch.clone(t.grad) for t in t1 if t.grad is not None]
    
        t1_c = tuple(torch.nn.Parameter(torch.clone(t)) for t in tensors)
        t1_c[-1].requires_grad_(False)
        loss_c = torch.cat(t1_c).sum()
        loss_c.backward()
&gt;       run_gradient_clipping(t1_c, max_norm)

tests/test_nn_utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

parameters = (Parameter containing:
tensor([[-1.1690, -1.0330,  0.1898, -0.9892,  0.2509],
        [-1.6767,  1.4562,  0.9681, -0.2....9786],
        [-0.3197,  1.8108, -1.2725, -1.1709,  0.8299],
        [ 2.0029,  1.7995, -0.2501,  0.1244, -0.4229]]))
max_l2_norm = 0.01

    def run_gradient_clipping(parameters: Iterable[torch.nn.Parameter], max_l2_norm: float) -&gt; None:
        """Given a set of parameters, clip their combined gradients to have l2 norm at most max_l2_norm.
    
        Args:
            parameters (Iterable[torch.nn.Parameter]): collection of trainable parameters.
            max_l2_norm (float): a positive value containing the maximum l2-norm.
    
        The gradients of the parameters (parameter.grad) should be modified in-place.
        """
&gt;       raise NotImplementedError
E       NotImplementedError

tests/adapters.py:312: NotImplementedError</failure></testcase><testcase classname="tests.test_optimizer" name="test_adamw" time="0.374"><failure message="NotImplementedError">numpy_snapshot = &lt;tests.conftest.NumpySnapshot object at 0x7ebd9eee2270&gt;

    def test_adamw(numpy_snapshot):
        """
        Our reference implementation yields slightly different results than the
        PyTorch AdamW, since there are a couple different ways that you can apply
        weight decay that are equivalent in principle, but differ in practice due to
        floating point behavior. So, we test that the provided implementation matches
        _either_ our reference implementation's expected results or those from the PyTorch AdamW.
        """
        # expected_weights = torch.load(FIXTURES_PATH / "adamw_expected_params.pt")
        pytorch_weights = _optimize(torch.optim.AdamW)
&gt;       actual_weights = _optimize(get_adamw_cls())
                                   ^^^^^^^^^^^^^^^

tests/test_optimizer.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def get_adamw_cls() -&gt; Any:
        """
        Returns a torch.optim.Optimizer that implements AdamW.
        """
&gt;       raise NotImplementedError
E       NotImplementedError

tests/adapters.py:319: NotImplementedError</failure></testcase><testcase classname="tests.test_optimizer" name="test_get_lr_cosine_schedule" time="0.000"><failure message="NotImplementedError">def test_get_lr_cosine_schedule():
        max_learning_rate = 1
        min_learning_rate = 1 * 0.1
        warmup_iters = 7
        cosine_cycle_iters = 21
    
        expected_lrs = [
            0,
            0.14285714285714285,
            0.2857142857142857,
            0.42857142857142855,
            0.5714285714285714,
            0.7142857142857143,
            0.8571428571428571,
            1.0,
            0.9887175604818206,
            0.9554359905560885,
            0.9018241671106134,
            0.8305704108364301,
            0.7452476826029011,
            0.6501344202803414,
            0.55,
            0.44986557971965857,
            0.3547523173970989,
            0.26942958916356996,
            0.19817583288938662,
            0.14456400944391146,
            0.11128243951817937,
            0.1,
            0.1,
            0.1,
            0.1,
        ]
        actual_lrs = [
&gt;           run_get_lr_cosine_schedule(
                it=it,
                max_learning_rate=max_learning_rate,
                min_learning_rate=min_learning_rate,
                warmup_iters=warmup_iters,
                cosine_cycle_iters=cosine_cycle_iters,
            )
            for it in range(25)
        ]

tests/test_optimizer.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

it = 0, max_learning_rate = 1, min_learning_rate = 0.1, warmup_iters = 7, cosine_cycle_iters = 21

    def run_get_lr_cosine_schedule(
        it: int,
        max_learning_rate: float,
        min_learning_rate: float,
        warmup_iters: int,
        cosine_cycle_iters: int,
    ):
        """
        Given the parameters of a cosine learning rate decay schedule (with linear
        warmup) and an iteration number, return the learning rate at the given
        iteration under the specified schedule.
    
        Args:
            it (int): Iteration number to get learning rate for.
            max_learning_rate (float): alpha_max, the maximum learning rate for
                cosine learning rate schedule (with warmup).
            min_learning_rate (float): alpha_min, the minimum / final learning rate for
                the cosine learning rate schedule (with warmup).
            warmup_iters (int): T_w, the number of iterations to linearly warm-up
                the learning rate.
            cosine_cycle_iters (int): T_c, the number of cosine annealing iterations.
    
        Returns:
            Learning rate at the given iteration under the specified schedule.
        """
&gt;       raise NotImplementedError
E       NotImplementedError

tests/adapters.py:347: NotImplementedError</failure></testcase><testcase classname="tests.test_serialization" name="test_checkpointing" time="0.009"><failure message="NotImplementedError">tmp_path = PosixPath('/tmp/pytest-of-smingtao01/pytest-0/test_checkpointing0')

    def test_checkpointing(tmp_path):
        torch.manual_seed(42)
        d_input = 100
        d_output = 10
        num_iters = 10
    
        model = _TestNet(d_input=d_input, d_output=d_output)
&gt;       optimizer = get_adamw_cls()(
                    ^^^^^^^^^^^^^^^
            model.parameters(),
            lr=1e-3,
            weight_decay=0.01,
            betas=(0.9, 0.999),
            eps=1e-8,
        )

tests/test_serialization.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def get_adamw_cls() -&gt; Any:
        """
        Returns a torch.optim.Optimizer that implements AdamW.
        """
&gt;       raise NotImplementedError
E       NotImplementedError

tests/adapters.py:319: NotImplementedError</failure></testcase><testcase classname="tests.test_tokenizer" name="test_roundtrip_empty" time="0.264" /><testcase classname="tests.test_tokenizer" name="test_empty_matches_tiktoken" time="15.472" /><testcase classname="tests.test_tokenizer" name="test_roundtrip_single_character" time="0.151" /><testcase classname="tests.test_tokenizer" name="test_single_character_matches_tiktoken" time="0.129" /><testcase classname="tests.test_tokenizer" name="test_roundtrip_single_unicode_character" time="0.128" /><testcase classname="tests.test_tokenizer" name="test_single_unicode_character_matches_tiktoken" time="0.119" /><testcase classname="tests.test_tokenizer" name="test_roundtrip_ascii_string" time="0.125" /><testcase classname="tests.test_tokenizer" name="test_ascii_string_matches_tiktoken" time="0.125" /><testcase classname="tests.test_tokenizer" name="test_roundtrip_unicode_string" time="0.136" /><testcase classname="tests.test_tokenizer" name="test_unicode_string_matches_tiktoken" time="0.129" /><testcase classname="tests.test_tokenizer" name="test_roundtrip_unicode_string_with_special_tokens" time="0.130" /><testcase classname="tests.test_tokenizer" name="test_unicode_string_with_special_tokens_matches_tiktoken" time="0.124" /><testcase classname="tests.test_tokenizer" name="test_overlapping_special_tokens" time="0.128" /><testcase classname="tests.test_tokenizer" name="test_address_roundtrip" time="0.118" /><testcase classname="tests.test_tokenizer" name="test_address_matches_tiktoken" time="0.119" /><testcase classname="tests.test_tokenizer" name="test_german_roundtrip" time="0.113" /><testcase classname="tests.test_tokenizer" name="test_german_matches_tiktoken" time="0.114" /><testcase classname="tests.test_tokenizer" name="test_tinystories_sample_roundtrip" time="0.119" /><testcase classname="tests.test_tokenizer" name="test_tinystories_matches_tiktoken" time="0.124" /><testcase classname="tests.test_tokenizer" name="test_encode_special_token_trailing_newlines" time="0.125" /><testcase classname="tests.test_tokenizer" name="test_encode_special_token_double_newline_non_whitespace" time="0.121" /><testcase classname="tests.test_tokenizer" name="test_encode_iterable_tinystories_sample_roundtrip" time="0.117" /><testcase classname="tests.test_tokenizer" name="test_encode_iterable_tinystories_matches_tiktoken" time="0.120" /><testcase classname="tests.test_tokenizer" name="test_encode_iterable_memory_usage" time="6.903" /><testcase classname="tests.test_tokenizer" name="test_encode_memory_usage" time="5.618"><skipped type="pytest.xfail" message="Tokenizer.encode is expected to take more memory than allotted (1MB)." /></testcase><testcase classname="tests.test_train_bpe" name="test_train_bpe_speed" time="0.990" /><testcase classname="tests.test_train_bpe" name="test_train_bpe" time="0.970" /><testcase classname="tests.test_train_bpe" name="test_train_bpe_special_tokens" time="4.273" /></testsuite></testsuites>