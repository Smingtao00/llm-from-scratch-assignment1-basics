{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195f199",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/e/bpe_data/lfs-data/owt-valid.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcs336_basics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbpe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. 训练 BPE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m vocab, merges = \u001b[43mbpe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/mnt/e/bpe_data/lfs-data/owt-valid.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|endoftext|>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|endoftext|><|endoftext|>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlongest token: \u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mmax\u001b[39m(vocab.items(), key = \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x[\u001b[32m1\u001b[39m])))\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2. 保存词汇表（vocab）\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Download/CS336/llm-from-scratch-assignment1-basics/cs336_basics/bpe.py:167\u001b[39m, in \u001b[36mbpe\u001b[39m\u001b[34m(input_path, vocab_size, special_tokens)\u001b[39m\n\u001b[32m    163\u001b[39m     vocab[next_id] = token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    164\u001b[39m     next_id += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    168\u001b[39m     num_processes = \u001b[32m8\u001b[39m\n\u001b[32m    169\u001b[39m     boundaries = find_chunk_boundaries(f, num_processes, special_tokens[\u001b[32m0\u001b[39m].encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/mnt/e/bpe_data/lfs-data/owt-valid.txt'"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe import *\n",
    "\n",
    "# 1. 训练 BPE\n",
    "vocab, merges = bpe(\n",
    "    input_path=\"/mnt/e/bpe_data/lfs-data/owt_valid.txt\",\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\", \"<|endoftext|><|endoftext|>\"],\n",
    ")\n",
    "\n",
    "print(\"longest token: \", max(vocab.items(), key = lambda x: len(x[1])))\n",
    "# 2. 保存词汇表（vocab）\n",
    "with open(\"/home/smingtao01/Download/CS336/llm-from-scratch-assignment1-basics/cs336_basics/owt_vocab.txt\", 'w', encoding=\"utf-8\") as f1:\n",
    "    for token_id, token_bytes in vocab.items():\n",
    "        # 将字节解码为字符串，处理解码错误\n",
    "        try:\n",
    "            token_str = token_bytes.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # 如果无法解码为 UTF-8，使用转义表示\n",
    "            token_str = repr(token_bytes)[2:-1]  # 去掉 b'...' 的 b'' 部分\n",
    "        \n",
    "        # 写入格式：ID<TAB>Token\n",
    "        f1.write(f\"{token_id}\\t{token_str}\\n\")\n",
    "\n",
    "print(f\"词汇表已保存: {len(vocab)} 个词元\")\n",
    "\n",
    "# 3. 保存合并规则（merges）\n",
    "with open(\"/home/smingtao01/Download/CS336/llm-from-scratch-assignment1-basics/cs336_basics/owt_merges.txt\", \"w\", encoding='utf-8') as f2:\n",
    "    # 检查 merges 的数据结构\n",
    "    # merges 可能是 dict 或 list，需要根据实际情况处理\n",
    "    \n",
    "    if isinstance(merges, dict):\n",
    "        # 如果是字典：{(b'a', b'b'): b'ab', ...}\n",
    "        for (byte1, byte2), merged_bytes in merges.items():\n",
    "            try:\n",
    "                pair_str = f\"{byte1.decode('utf-8')} + {byte2.decode('utf-8')}\"\n",
    "                merged_str = merged_bytes.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                pair_str = f\"{repr(byte1)} + {repr(byte2)}\"\n",
    "                merged_str = repr(merged_bytes)\n",
    "            \n",
    "            f2.write(f\"{pair_str} -> {merged_str}\\n\")\n",
    "    \n",
    "    elif isinstance(merges, list):\n",
    "        # 如果是列表：[(b'a', b'b'), (b'b', b'c'), ...]\n",
    "        for i, (byte1, byte2) in enumerate(merges, 1):\n",
    "            try:\n",
    "                pair_str = f\"{byte1.decode('utf-8')} {byte2.decode('utf-8')}\"\n",
    "            except UnicodeDecodeError:\n",
    "                pair_str = f\"{repr(byte1)} {repr(byte2)}\"\n",
    "            \n",
    "            f2.write(f\"{i}: {pair_str}\\n\")\n",
    "\n",
    "print(f\"合并规则已保存: {len(merges)} 个规则\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
